{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b08ed96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda using PyTorch 2.0.1+cu117 and Flower 1.4.0\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.common import Metrics\n",
    "\n",
    "DEVICE = torch.device(\"cuda\")  # Try \"cuda\" to train on GPU\n",
    "print(\n",
    "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08ee016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3,4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfd0991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from src.peft import get_peft_config, get_peft_model, PeftConfig, LoraConfig, PeftModel, TaskType, PrefixTuningConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1622b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = \"google/Flan-t5-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "537b3ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_config = PrefixTuningConfig(task_type=TaskType.SEQ_2_SEQ_LM, \n",
    "#                                  inference_mode=False, \n",
    "#                                  prefix_projection = True,\n",
    "#                                  num_virtual_tokens=30)\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT)\n",
    "# model = get_peft_model(model, peft_config)\n",
    "# model.print_trainable_parameters()\n",
    "# print(model.prompt_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fba59834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_config\n",
    "# model.prompt_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaf5339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7cb8f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccc3f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers import logging\n",
    "from datasets import load_dataset\n",
    "from evaluate import load as load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b55edc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "#transform pandas type to Dataset for easy implement\n",
    "# train = Dataset.from_pandas(df_train)\n",
    "# val = Dataset.from_pandas(df_val)\n",
    "# test = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed626b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a49ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     df = pd.read_csv(f\"./clients/client_{i}/train.csv\")\n",
    "#     df = df.dropna(subset = [\"Input\"])\n",
    "#     df = df.dropna(subset = [\"Label\"])\n",
    "#     df.to_csv(f\"./clients/client_{i}/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efbc30da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     df = pd.read_csv(f\"./clients/client_{i}/test.csv\")\n",
    "#     df = df.dropna(subset = [\"Input\"])\n",
    "#     df = df.dropna(subset = [\"Label\"])\n",
    "#     df.to_csv(f\"./clients/client_{i}/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1e52b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     df = pd.read_csv(f\"./clients/client_{i}/test.csv\")\n",
    "#     df = df[['Input', 'Label', 'assigned']]\n",
    "#     print(df.columns)\n",
    "#     df.to_csv(f\"./clients/client_{i}/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "396f68c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 64\n",
    "max_target_length = 30\n",
    "def preprocess_function(examples):\n",
    "    try :\n",
    "        model_inputs = tokenizer(\n",
    "            examples[\"Input\"],\n",
    "            max_length=max_input_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        \n",
    "        labels = tokenizer(\n",
    "            examples[\"Label\"], \n",
    "            max_length=max_target_length, \n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    except :\n",
    "        pdb.set_trace()\n",
    "        \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b6cf3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = {\"Input\" : str, 'Label' : str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c8c338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "def load_data(clients_num):\n",
    "    trainloader = []\n",
    "    testloader = []\n",
    "    \n",
    "    dataset_train = [Dataset.from_pandas(pd.read_csv(f\"./clients/client_{i}/train.csv\",dtype=data_types, usecols = [\"Input\", \"Label\"])) for i in range(clients_num)]\n",
    "    dataset_test = [Dataset.from_pandas(pd.read_csv(f\"./clients/client_{i}/test.csv\",dtype=data_types, usecols = [\"Input\", \"Label\"])) for i in range(clients_num)]\n",
    "    \n",
    "\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT,padding=True,truncation=True)\n",
    "\n",
    "\n",
    "    # Select 20 random samples to reduce the computation cost\n",
    "    \n",
    "#     tokenized_datasets[\"train\"] = tokenized_datasets[\"train\"].select(train_population)\n",
    "#     tokenized_datasets[\"test\"] = tokenized_datasets[\"test\"].select(test_population)\n",
    "#     tokenized_datasets = tokenized_datasets.remove_columns(\"text\")\n",
    "#     tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "#     data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "    tokenized_train = [dataset.map(preprocess_function) for dataset in dataset_train]\n",
    "    tokenized_test = [dataset.map(preprocess_function) for dataset in dataset_test]\n",
    "        \n",
    "    \n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)    \n",
    "    for dataset in tokenized_train:\n",
    "        dataset = dataset.remove_columns([\"Input\", \"Label\"])\n",
    "        \n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            shuffle=True,\n",
    "            batch_size=8,\n",
    "            collate_fn=data_collator,\n",
    "        )\n",
    "        \n",
    "        trainloader.append(loader)\n",
    "\n",
    "        \n",
    "        \n",
    "    for dataset in tokenized_test:\n",
    "        dataset = dataset.remove_columns([\"Input\", \"Label\"])\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=8,\n",
    "            collate_fn=data_collator,\n",
    "        )\n",
    "        \n",
    "        testloader.append(loader)\n",
    "        \n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19c12321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 1\n",
    "# train_set = Dataset.from_pandas(pd.read_csv(f\"./clients/client_{i}/train.csv\",dtype=data_types, usecols = [\"Input\", \"Label\"]))\n",
    "# train_set = train_set.map(preprocess_function)\n",
    "# train_set = train_set.remove_columns([\"Input\", \"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1feb57e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d61b2555574429a202eb204ab6173c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b86de863e904b90a201919c13360d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15869 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c01aeee13449a5a29b5589b8d28538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7683 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a4cce3ecac421e86f8db56521c51f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11336 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9e75740b534518a6ba0e907715984d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40cad15c4c74695a58285bf599bee77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1882 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a0959e81b44f158b02e185014709a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3967 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1e5aa1ba3c49fda0d9ef5004fc7c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1921 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87fb479868a4eecb443327ac12ee53a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2835 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534881f3e2b8400bbe5b9c5ca1cdcab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2518 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainloaders, testloaders = load_data(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fde2ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import evaluate\n",
    "import pdb\n",
    "\n",
    "def train(net, trainloader, epochs):\n",
    "    optimizer = AdamW(net.parameters(), lr=5e-5)\n",
    "    net.train()\n",
    "    losses = 0\n",
    "    for _ in range(epochs):\n",
    "        for batch in trainloader:\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "            outputs = net(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses += loss.item()\n",
    "        \n",
    "        print(f\"loss : {loss / len(trainloader)}\")\n",
    "\n",
    "def test(net, testloader):\n",
    "    rouge_score = evaluate.load(\"rouge\")\n",
    "    loss = 0\n",
    "    net.eval()\n",
    "    score_list = []\n",
    "    for batch in testloader:\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        labels = batch['labels'].cpu()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = net.generate(**batch, max_length = 70)\n",
    "        \n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "        decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "        rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)       \n",
    "        \n",
    "        \n",
    "    # Extract the median ROUGE scores\n",
    "    result = rouge_score.compute()\n",
    "    print(decoded_preds)\n",
    "    print(decoded_labels)\n",
    "    result = {key: value for key, value in result.items()}\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result['rouge2'], result['rouge2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f84e129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm terribly sorry old man.\\nI'm sorry.\", \"Yeah so?\\nThe guy's beat up - he...he probably has a concussion or something, right?\\nHe's a snort...he's a snort...he's a snort...he's a sn\"]\n",
      "[\"I'm terribly sorry old man.\", \"Yeah so?\\nThe guy's beat up - he...he probably has a concussion or something, right?\\nHe\"]\n",
      "[\"That'll be the day... 'As we developed the village'... Next time you develop a village, hit it where the fucking fucking fucking fucking fucking fucking fucking fucking fucking fucking\", 'Once they gave me a job to draw up an estimate for the construction of a school.\\nIt took me only a day and a half to get the job done.', \"Ha Ha!\\nYou said, I do.\\nI guess that means we're married.\\nI'm not sure.\", \"Yeesh, they sound like a cult.\\nBesides, they're greedy bitches.\\nThey already have three children.\", \"Well, I haven't fought just one person for so long.\\nI've been specializing in groups.\\nBattling a group of people is a way to get them to get out of the way.\"]\n",
      "[\"That'll be the day... 'As we developed the village'... Next time you develop a village, hit it where the\", 'Once they gave me a job to draw up an estimate for the construction of a school.\\nIt took me only a day and', \"Ha Ha!\\nYou said, I do.\\nI guess that means we're married.\", \"Yeesh, they sound like a cult.\\nBesides, they're greedy bitches.\\nThey already have three\", \"Well, I haven't fought just one person for so long.\\nI've been specializing in groups.\\nBattling\"]\n",
      "[\"Curious?\\nThat's funny, I'm feeling a bit curious myself right now.\", 'Come to me, boy!\\nCome to me, little bird.', \"Norman Rockwell's 'Homecoming.'\\n'cause I'm a bit confused.\"]\n",
      "[\"Curious?\\nThat's funny, I'm feeling a bit curious myself right now.\", 'Come to me, boy!', \"Norman Rockwell's 'Homecoming.'\"]\n",
      "['No dog.\\nthree of four times', \"When will they be back?\\ni'm afraid they are out of the game\", \"Son, lots of people tell me I'm a gifted man, but I still can't see around corners.\\nI'm not sure\", \"Well, I wasn't going to say that -- i'm your daughter.\", \"I dont.\\nI don't know what you like.\", \"Just forget that.\\nForget what it says in the book.\\nYou're right.\", \"I'm not sure I believe.\\nBut then I had no one to instruct me.\\nI had no mother... and my father abandoned me at the door.\", \"Sure.\\nI'm not sure.\"]\n",
      "['No dog.', 'When will they be back?', \"Son, lots of people tell me I'm a gifted man, but I still can't see around corners.\", \"Well, I wasn't going to say that --\", 'I dont.', 'Just forget that.\\nForget what it says in the book.', \"I'm not sure I believe.\\nBut then I had no one to instruct me.\\nI had no mother... and my father abandoned me at\", 'Sure.']\n",
      "['Helloo... you let him have anal sex with you.\\nHelloo... you let him videotape you diddling yourself.', '\"The Strawberries\"?\\nI can\\'t yell \"Oh, butler,\" can I?', \"Half a buck I'd put him in jail... he should be put in jail\", \"You bring trunks, Jack?\\nI'll take her inside.\", \"New York...\\nI'm a cop from New York.\", 'They just ran back out into Death Valley -- you know, where Manson turned up...\\nI do.', \"Go fuck yourself.\\ni don't know.\"]\n",
      "['Helloo... you let him have anal sex with you.\\nHelloo... you let him videotape you diddling yourself.', '\"The Strawberries\"?', \"Half a buck I'd put him in jail...\", 'You bring trunks, Jack?', 'New York...', 'They just ran back out into Death Valley -- you know, where Manson turned up...', 'Go fuck yourself.']\n"
     ]
    }
   ],
   "source": [
    "# train(model.to(DEVICE), trainloaders[0], 1)\n",
    "res_list = []\n",
    "for i in range(5):\n",
    "    res = test(model.to(DEVICE), trainloaders[i])\n",
    "    res_list.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34655eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7130600000000001"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = 0\n",
    "for i in res_list:\n",
    "    ss += i[0]\n",
    "\n",
    "ss/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73cf62e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBClient(fl.client.NumPyClient):\n",
    "    def __init__(self, net, trainloader, testloader):\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return [val.cpu().numpy() for _, val in self.net.state_dict().items()]\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        params_dict = zip(self.net.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "        self.net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        print(\"Training Started...\")\n",
    "        train(self.net, self.trainloader, epochs=1)\n",
    "        print(\"Training Finished.\")\n",
    "        return self.get_parameters(config={}), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        loss, accuracy = test(self.net, self.testloader)\n",
    "        return float(loss), len(self.testloader), {\"accuracy\": float(accuracy), \"loss\": float(loss)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dab51ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefixClient(fl.client.NumPyClient):\n",
    "    def __init__(self, net, trainloader, testloader):\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return self.net.get_prompt_parameters()\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        self.net.set_prompt_parameters(parameters)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        print(\"Training Started...\")\n",
    "        train(self.net, self.trainloader, epochs=1)\n",
    "        print(\"Training Finished.\")\n",
    "        return self.get_parameters(config={}), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        loss, accuracy = test(self.net, self.testloader)\n",
    "        return float(loss), len(self.testloader), {\"accuracy\": float(accuracy), \"loss\": float(loss)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea4178f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_fn(cid: str) -> IMDBClient:\n",
    "    \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
    "\n",
    "    # Load model\n",
    "    net = model.to(DEVICE)\n",
    "\n",
    "    # Load data (CIFAR-10)\n",
    "    # Note: each client gets a different trainloader/valloader, so each client\n",
    "    # will train and evaluate on their own unique data\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = testloaders[int(cid)]\n",
    "\n",
    "    # Create a  single Flower client representing a single organization\n",
    "    return IMDBClient(net, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee631e8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-05 13:49:00,477 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-05 13:49:03,282\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "INFO flwr 2023-08-05 13:49:04,706 | app.py:180 | Flower VCE: Ray initialized with resources: {'object_store_memory': 29529923174.0, 'GPU': 6.0, 'memory': 59059846350.0, 'accelerator_type:RTX': 1.0, 'CPU': 40.0, 'node:__internal_head__': 1.0, 'node:202.30.1.208': 1.0}\n",
      "INFO flwr 2023-08-05 13:49:04,708 | server.py:86 | Initializing global parameters\n",
      "INFO flwr 2023-08-05 13:49:04,710 | server.py:273 | Requesting initial parameters from one random client\n",
      "INFO flwr 2023-08-05 13:49:25,151 | server.py:277 | Received initial parameters from one random client\n",
      "INFO flwr 2023-08-05 13:49:25,154 | server.py:88 | Evaluating initial parameters\n",
      "INFO flwr 2023-08-05 13:49:25,155 | server.py:101 | FL starting\n",
      "DEBUG flwr 2023-08-05 13:49:25,157 | server.py:218 | fit_round 1: strategy sampled 5 clients (out of 5)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=38394)\u001b[0m /home/qudgks/miniconda3/envs/FL/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=38394)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=38394)\u001b[0m You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=38394)\u001b[0m Training Started...\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=38481)\u001b[0m loss : 0.0036469155456870794\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=38481)\u001b[0m Training Finished.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=38496)\u001b[0m Training Started...\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=38437)\u001b[0m loss : 0.003442393383011222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=38437)\u001b[0m Training Finished.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=38496)\u001b[0m loss : 0.002363845007494092\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=38496)\u001b[0m Training Finished.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=38394)\u001b[0m loss : 0.0023197701666504145\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=38394)\u001b[0m Training Finished.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=38577)\u001b[0m loss : 0.001842861413024366\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=38577)\u001b[0m Training Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-05 13:56:59,259 | server.py:232 | fit_round 1 received 5 results and 0 failures\n",
      "WARNING flwr 2023-08-05 13:57:23,265 | fedavg.py:243 | No fit_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2023-08-05 13:57:23,298 | server.py:168 | evaluate_round 1: strategy sampled 5 clients (out of 5)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=38496)\u001b[0m /home/qudgks/miniconda3/envs/FL/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=38496)\u001b[0m   warnings.warn(\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=3857)\u001b[0m You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=3960)\u001b[0m ['Uh-huh!']\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=3960)\u001b[0m ['Uh-huh!']\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=3961)\u001b[0m [\"I know.\\nI'm sorry.\\nI'm sorry.\\nI'm sorry.\\nI'm sorry.\", 'Nonsense No.']\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=3961)\u001b[0m [\"I know.\\nI'm sorry.\", 'Nonsense']\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=3857)\u001b[0m [\"Other than Brill.\\nI'm not sure.\", \"No, you guys are great together.\\nHe'll come through, I'm sure.\\nI'm not.\", \"I don't care what you believe.\\nI saved your life once, I don't plan on making a habit of it.\\nYou're a shithole.\"]\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=3857)\u001b[0m ['Other than Brill.', \"No, you guys are great together.\\nHe'll come through, I'm sure.\", \"I don't care what you believe.\\nI saved your life once, I don't plan on making a habit of it.\\nYou\"]\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=3890)\u001b[0m [\"Funnier to who and at who's expense?\\nDunwitty, when Negroes start to run amok, the boycotts are a thing of the past.\", \"Yes, with your clients' names attached.\\nThat's the only reason those poor slobs pay you - to see their names.\", \"Dr. Bloom showed me your article on surgical addiction in the journal of Clinical Psychiatry.\\nI'm sorry, I'm sorry.\", \"It's the truth.\\nThose 17 superstars are our insurance policy.\\nWe can't open -- can't make a profit --\", \"Losing power.\\nThe laser drained it.\\nI'm sorry, I'm sorry.\", \"You mock us with a smile?\\nI'm not sure.\", \"I've got my Dr. Gruber, says her heart condition means they gave her the wrong anaesthetic anyway, plus she's a sex addict.\"]\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=3890)\u001b[0m [\"Funnier to who and at who's expense?\\nDunwitty, when Negroes start to run amok, the boycotts\", \"Yes, with your clients' names attached.\\nThat's the only reason those poor slobs pay you - to see their names\", 'Dr. Bloom showed me your article on surgical addiction in the journal of Clinical Psychiatry.', \"It's the truth.\\nThose 17 superstars are our insurance policy.\\nWe can't open -- can't make a profit\", 'Losing power.\\nThe laser drained it.', 'You mock us with a smile?', \"I've got my Dr. Gruber, says her heart condition means they gave her the wrong anaesthetic anyway, plus she\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-05 14:03:13,170 | server.py:182 | evaluate_round 1 received 5 results and 0 failures\n",
      "WARNING flwr 2023-08-05 14:03:13,172 | fedavg.py:274 | No evaluate_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2023-08-05 14:03:13,174 | server.py:218 | fit_round 2: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=3952)\u001b[0m [\"... That's funny...\\nI had this job with Enzo... we got fired too.\", \"Five and a half minutes.\\nHe's breached the hot zone..\\nI'm not sure.\", \"No.\\nNever heard of him.\\nI'm not sure.\", \"A what?\\nI'm a paleontologist.\", \"What was the color of Tim's belt?\\nI'm not sure.\\nI'm not sure.\", \"No.\\nHeadin' west.\\nTo Can-tuck-ee.\\nI'm not sure.\"]\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=3952)\u001b[0m [\"... That's funny...\\nI had this job with Enzo... we got fired too.\", \"Five and a half minutes.\\nHe's breached the hot zone..\", 'No.\\nNever heard of him.', 'A what?', \"What was the color of Tim's belt?\", \"No.\\nHeadin' west.\\nTo Can-tuck-ee.\"]\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=8601)\u001b[0m Training Started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=8601)\u001b[0m /home/qudgks/miniconda3/envs/FL/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=8601)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=8601)\u001b[0m You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=8650)\u001b[0m /home/qudgks/miniconda3/envs/FL/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=8650)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=8601)\u001b[0m loss : 0.00420457823202014\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=8601)\u001b[0m Training Finished.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=8703)\u001b[0m Training Started...\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=8709)\u001b[0m loss : 0.003306831931695342\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=8709)\u001b[0m Training Finished.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=8708)\u001b[0m loss : 0.0031282224226742983\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=8708)\u001b[0m Training Finished.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=8650)\u001b[0m loss : 0.002395196119323373\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=8650)\u001b[0m Training Finished.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=8703)\u001b[0m loss : 0.0020141578279435635\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=8703)\u001b[0m Training Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-05 14:10:30,188 | server.py:232 | fit_round 2 received 5 results and 0 failures\n",
      "DEBUG flwr 2023-08-05 14:10:43,366 | server.py:168 | evaluate_round 2: strategy sampled 5 clients (out of 5)\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=13837)\u001b[0m You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=8703)\u001b[0m /home/qudgks/miniconda3/envs/FL/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=8703)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=13875)\u001b[0m ['Uh-huh!']\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=13875)\u001b[0m ['Uh-huh!']\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=13879)\u001b[0m [\"I know.\\nI'm sorry.\\nI'm sorry.\\nI'm sorry.\\nI'm sorry.\\nI'm sorry.\", \"Nonsense No, I don't know.\"]\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=13879)\u001b[0m [\"I know.\\nI'm sorry.\", 'Nonsense']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=18286)\u001b[0m You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18286)\u001b[0m /home/qudgks/miniconda3/envs/FL/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18286)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=18286)\u001b[0m Training Started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=18332)\u001b[0m /home/qudgks/miniconda3/envs/FL/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18332)\u001b[0m   warnings.warn(\n",
      "ERROR flwr 2023-08-05 14:17:42,951 | ray_client_proxy.py:87 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 202.30.1.208, ID: ae1cbb782a557cd6836f236eb217d718833893871463856700c995d6) where the task (task ID: 843902242f17ef64d03453d51662d230b8de25be01000000, name=launch_and_fit, pid=18372, memory used=4.90GB) was running was 119.33GB / 125.55GB (0.950422), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dd4f11eee880c496189eebb5507d6e037fd0d3be0477e9dfbf7a806) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 202.30.1.208`. To see the logs of the worker, use `ray logs worker-2dd4f11eee880c496189eebb5507d6e037fd0d3be0477e9dfbf7a806*out -ip 202.30.1.208. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "35941\t40.03\t/home/qudgks/miniconda3/envs/FL/bin/python -m ipykernel_launcher -f /home/qudgks/.local/share/jupyte...\n",
      "27839\t5.53\twandb-service(2-27767-s-39937)\n",
      "34365\t4.95\twandb-service(2-34292-s-34335)\n",
      "18286\t4.90\tray::launch_and_fit\n",
      "18362\t4.90\tray::launch_and_fit\n",
      "18372\t4.90\tray::launch_and_fit\n",
      "18332\t4.90\tray::launch_and_fit\n",
      "18363\t4.90\tray::launch_and_fit\n",
      "37816\t3.26\tray::IDLE\n",
      "29159\t0.43\ttmux\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-08-05 14:18:03,249 E 36800 36800] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: ae1cbb782a557cd6836f236eb217d718833893871463856700c995d6, IP: 202.30.1.208) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 202.30.1.208`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=18362)\u001b[0m loss : 0.001979973865672946\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18362)\u001b[0m Training Finished.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18363)\u001b[0m Training Started...\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18332)\u001b[0m loss : 0.0036319708451628685\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18332)\u001b[0m Training Finished.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18286)\u001b[0m loss : 0.0025100158527493477\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18286)\u001b[0m Training Finished.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18363)\u001b[0m loss : 0.001948125078342855\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18363)\u001b[0m Training Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-05 14:24:12,680 | server.py:232 | fit_round 3 received 4 results and 1 failures\n",
      "DEBUG flwr 2023-08-05 14:24:21,268 | server.py:168 | evaluate_round 3: strategy sampled 5 clients (out of 5)\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=23877)\u001b[0m You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18363)\u001b[0m /home/qudgks/miniconda3/envs/FL/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18363)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=23947)\u001b[0m [\"I know.\\nI'm sorry.\\nI'm sorry.\\nI'm sorry.\\nI'm sorry.\\nI'm sorry.\\nI'm sorry.\", \"Nonsense No, I'm not.\"]\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=23947)\u001b[0m [\"I know.\\nI'm sorry.\", 'Nonsense']\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=23945)\u001b[0m ['Uh-huh!']\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=23945)\u001b[0m ['Uh-huh!']\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=23877)\u001b[0m [\"Funnier to who and at who's expense?\\nDunwitty, when Negroes start to run amok, the boycotts\", \"Yes, with your clients' names attached.\\nThat's the only reason those poor slobs pay you - to see their names\", \"Dr. Bloom showed me your article on surgical addiction in the journal of Clinical Psychiatry.\\nI'm sorry, I'm sorry.\", \"It's the truth.\\nThose 17 superstars are our insurance policy.\\nWe can't open -- can't make a profit --\", \"Losing power.\\nThe laser drained it.\\nI'm sorry, Mr. Sullivan.\", \"You mock us with a smile?\\nI'm sorry, Mr. Smith.\", \"I've got my Dr. Gruber, says her heart condition means they gave her the wrong anaesthetic anyway, plus she's\"]\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=23877)\u001b[0m [\"Funnier to who and at who's expense?\\nDunwitty, when Negroes start to run amok, the boycotts\", \"Yes, with your clients' names attached.\\nThat's the only reason those poor slobs pay you - to see their names\", 'Dr. Bloom showed me your article on surgical addiction in the journal of Clinical Psychiatry.', \"It's the truth.\\nThose 17 superstars are our insurance policy.\\nWe can't open -- can't make a profit\", 'Losing power.\\nThe laser drained it.', 'You mock us with a smile?', \"I've got my Dr. Gruber, says her heart condition means they gave her the wrong anaesthetic anyway, plus she\"]\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=23911)\u001b[0m [\"Other than Brill.\\nI'm sorry, Mr. Brill.\", \"No, you guys are great together.\\nHe'll come through, I'm sure.\\nI'm sorry.\", \"I don't care what you believe.\\nI saved your life once, I don't plan on making a habit of it.\\nYou\"]\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=23911)\u001b[0m ['Other than Brill.', \"No, you guys are great together.\\nHe'll come through, I'm sure.\", \"I don't care what you believe.\\nI saved your life once, I don't plan on making a habit of it.\\nYou\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-05 14:31:34,239 | server.py:182 | evaluate_round 3 received 5 results and 0 failures\n",
      "DEBUG flwr 2023-08-05 14:31:34,242 | server.py:218 | fit_round 4: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=23956)\u001b[0m [\"... That's funny...\\nI had this job with Enzo... we got fired too.\", \"Five and a half minutes.\\nHe's breached the hot zone..\\nI'm not sure.\", \"No.\\nNever heard of him.\\nI'm not sure.\", \"A what?\\nI'm a paleontologist.\", \"What was the color of Tim's belt?\\nI'm sorry, I'm sorry, I'm sorry.\\nI'm sorry, I'm sorry.\\nI'm sorry.\\nI'm sorry.\\nI'm sorry.\\nI'm sorry.\\nI'm sorry\", \"No.\\nHeadin' west.\\nTo Can-tuck-ee.\\nI'm not a shit.\"]\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=23956)\u001b[0m [\"... That's funny...\\nI had this job with Enzo... we got fired too.\", \"Five and a half minutes.\\nHe's breached the hot zone..\", 'No.\\nNever heard of him.', 'A what?', \"What was the color of Tim's belt?\", \"No.\\nHeadin' west.\\nTo Can-tuck-ee.\"]\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=29362)\u001b[0m Training Started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=29362)\u001b[0m You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=29362)\u001b[0m /home/qudgks/miniconda3/envs/FL/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=29362)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=29399)\u001b[0m /home/qudgks/miniconda3/envs/FL/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=29399)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=29398)\u001b[0m loss : 0.003506239503622055\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=29398)\u001b[0m Training Finished.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=29398)\u001b[0m Training Started...\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2023-08-05 14:35:34,185 | ray_client_proxy.py:87 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 202.30.1.208, ID: ae1cbb782a557cd6836f236eb217d718833893871463856700c995d6) where the task (task ID: aed910ef068462814962d550224f77ef375596fa01000000, name=launch_and_fit, pid=29407, memory used=4.90GB) was running was 119.44GB / 125.55GB (0.951296), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d39044dfafe8d269d67052c48784bae7fb0bc5e56fcc92b7d2fdf92) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 202.30.1.208`. To see the logs of the worker, use `ray logs worker-7d39044dfafe8d269d67052c48784bae7fb0bc5e56fcc92b7d2fdf92*out -ip 202.30.1.208. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "35941\t37.69\t/home/qudgks/miniconda3/envs/FL/bin/python -m ipykernel_launcher -f /home/qudgks/.local/share/jupyte...\n",
      "29398\t6.79\tray::launch_and_fit\n",
      "27839\t5.53\twandb-service(2-27767-s-39937)\n",
      "34365\t4.95\twandb-service(2-34292-s-34335)\n",
      "29407\t4.90\tray::launch_and_fit\n",
      "29401\t4.90\tray::launch_and_fit\n",
      "29399\t4.90\tray::launch_and_fit\n",
      "29362\t4.90\tray::launch_and_fit\n",
      "37816\t3.26\tray::IDLE\n",
      "29159\t0.43\ttmux\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=29399)\u001b[0m loss : 0.003731293836608529\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=29399)\u001b[0m Training Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-08-05 14:36:03,299 E 36800 36800] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: ae1cbb782a557cd6836f236eb217d718833893871463856700c995d6, IP: 202.30.1.208) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 202.30.1.208`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=29362)\u001b[0m loss : 0.002389580477029085\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=29362)\u001b[0m Training Finished.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=29401)\u001b[0m loss : 0.0015749313170090318\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=29401)\u001b[0m Training Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-05 14:38:51,279 | server.py:232 | fit_round 4 received 4 results and 1 failures\n",
      "DEBUG flwr 2023-08-05 14:38:59,226 | server.py:168 | evaluate_round 4: strategy sampled 5 clients (out of 5)\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=34823)\u001b[0m You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=29398)\u001b[0m /home/qudgks/miniconda3/envs/FL/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=29398)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=34884)\u001b[0m [\"I know.\\nI'm sorry.\\nI'm sorry.\\nI'm sorry.\\nI'm sorry.\\nI'm sorry.\", 'Nonsense No, no.']\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=34884)\u001b[0m [\"I know.\\nI'm sorry.\", 'Nonsense']\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=34879)\u001b[0m ['Uh-huh!']\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=34879)\u001b[0m ['Uh-huh!']\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=34873)\u001b[0m [\"Funnier to who and at who's expense?\\nDunwitty, when Negroes start to run amok, the boycotts\", \"Yes, with your clients' names attached.\\nThat's the only reason those poor slobs pay you - to see their names.\", \"Dr. Bloom showed me your article on surgical addiction in the journal of Clinical Psychiatry.\\nI'm sorry, I'm sorry.\", \"It's the truth.\\nThose 17 superstars are our insurance policy.\\nWe can't open -- can't make a profit --\", \"Losing power.\\nThe laser drained it.\\nI'm sorry, Mr. Sullivan.\", \"You mock us with a smile?\\nI'm not a snob.\", \"I've got my Dr. Gruber, says her heart condition means they gave her the wrong anaesthetic anyway, plus she's\"]\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=34873)\u001b[0m [\"Funnier to who and at who's expense?\\nDunwitty, when Negroes start to run amok, the boycotts\", \"Yes, with your clients' names attached.\\nThat's the only reason those poor slobs pay you - to see their names\", 'Dr. Bloom showed me your article on surgical addiction in the journal of Clinical Psychiatry.', \"It's the truth.\\nThose 17 superstars are our insurance policy.\\nWe can't open -- can't make a profit\", 'Losing power.\\nThe laser drained it.', 'You mock us with a smile?', \"I've got my Dr. Gruber, says her heart condition means they gave her the wrong anaesthetic anyway, plus she\"]\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=34823)\u001b[0m [\"Other than Brill.\\nI'm not sure.\", \"No, you guys are great together.\\nHe'll come through, I'm sure.\\nI'm not wasting my time.\", \"I don't care what you believe.\\nI saved your life once, I don't plan on making a habit of it.\\nYou're\"]\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=34823)\u001b[0m ['Other than Brill.', \"No, you guys are great together.\\nHe'll come through, I'm sure.\", \"I don't care what you believe.\\nI saved your life once, I don't plan on making a habit of it.\\nYou\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-05 14:46:05,549 | server.py:182 | evaluate_round 4 received 5 results and 0 failures\n",
      "DEBUG flwr 2023-08-05 14:46:05,552 | server.py:218 | fit_round 5: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=34872)\u001b[0m [\"... That's funny...\\nI had this job with Enzo... we got fired too.\", \"Five and a half minutes.\\nHe's breached the hot zone..\\nI'm not gonna be able to get him.\", \"No.\\nNever heard of him.\\nI'm not sure.\", \"A what?\\nI'm a paleontologist.\", \"What was the color of Tim's belt?\\nI'm not a shithole.\\nI'm a shithole.\\nI'm a shithole.\\nI'm a shithole.\", \"No.\\nHeadin' west.\\nTo Can-tuck-ee.\\nI'm not a snob.\"]\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=34872)\u001b[0m [\"... That's funny...\\nI had this job with Enzo... we got fired too.\", \"Five and a half minutes.\\nHe's breached the hot zone..\", 'No.\\nNever heard of him.', 'A what?', \"What was the color of Tim's belt?\", \"No.\\nHeadin' west.\\nTo Can-tuck-ee.\"]\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=617)\u001b[0m Training Started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=617)\u001b[0m You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=617)\u001b[0m /home/qudgks/miniconda3/envs/FL/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=617)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=693)\u001b[0m /home/qudgks/miniconda3/envs/FL/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=693)\u001b[0m   warnings.warn(\n",
      "ERROR flwr 2023-08-05 14:46:56,268 | ray_client_proxy.py:87 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 202.30.1.208, ID: ae1cbb782a557cd6836f236eb217d718833893871463856700c995d6) where the task (task ID: 066cf17837cf236e255fb223385ed433e674eea501000000, name=launch_and_fit, pid=703, memory used=4.90GB) was running was 119.28GB / 125.55GB (0.950011), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d0cbb3810fc41de8f2377dd493838c518f42c11ffb61abc81f807663) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 202.30.1.208`. To see the logs of the worker, use `ray logs worker-d0cbb3810fc41de8f2377dd493838c518f42c11ffb61abc81f807663*out -ip 202.30.1.208. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "35941\t39.74\t/home/qudgks/miniconda3/envs/FL/bin/python -m ipykernel_launcher -f /home/qudgks/.local/share/jupyte...\n",
      "27839\t5.53\twandb-service(2-27767-s-39937)\n",
      "34365\t4.95\twandb-service(2-34292-s-34335)\n",
      "695\t4.91\tray::launch_and_fit\n",
      "693\t4.90\tray::launch_and_fit\n",
      "660\t4.90\tray::launch_and_fit\n",
      "617\t4.90\tray::launch_and_fit\n",
      "703\t4.90\tray::launch_and_fit\n",
      "37816\t3.26\tray::IDLE\n",
      "29159\t0.43\ttmux\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-08-05 14:47:03,329 E 36800 36800] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: ae1cbb782a557cd6836f236eb217d718833893871463856700c995d6, IP: 202.30.1.208) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 202.30.1.208`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=660)\u001b[0m loss : 0.004065924324095249\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=660)\u001b[0m Training Finished.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=703)\u001b[0m Training Started...\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=617)\u001b[0m loss : 0.0026054889895021915\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=617)\u001b[0m Training Finished.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=695)\u001b[0m loss : 0.0022846651263535023\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=695)\u001b[0m Training Finished.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=693)\u001b[0m loss : 0.0015613093273714185\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=693)\u001b[0m Training Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-05 14:53:19,999 | server.py:232 | fit_round 5 received 4 results and 1 failures\n",
      "DEBUG flwr 2023-08-05 14:53:31,438 | server.py:168 | evaluate_round 5: strategy sampled 5 clients (out of 5)\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=7138)\u001b[0m You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=703)\u001b[0m /home/qudgks/miniconda3/envs/FL/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=703)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=7171)\u001b[0m [\"Funnier to who and at who's expense?\\nDunwitty, when Negroes start to run amok, the boycotts\", \"Yes, with your clients' names attached.\\nThat's the only reason those poor slobs pay you - to see their names\", \"Dr. Bloom showed me your article on surgical addiction in the journal of Clinical Psychiatry.\\nI'm sorry, I'm sorry.\", \"It's the truth.\\nThose 17 superstars are our insurance policy.\\nWe can't open -- can't make a profit --\", \"Losing power.\\nThe laser drained it.\\nI'm not sure.\", \"You mock us with a smile?\\nI'm not a snob.\", \"I've got my Dr. Gruber, says her heart condition means they gave her the wrong anaesthetic anyway, plus she's\"]\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=7171)\u001b[0m [\"Funnier to who and at who's expense?\\nDunwitty, when Negroes start to run amok, the boycotts\", \"Yes, with your clients' names attached.\\nThat's the only reason those poor slobs pay you - to see their names\", 'Dr. Bloom showed me your article on surgical addiction in the journal of Clinical Psychiatry.', \"It's the truth.\\nThose 17 superstars are our insurance policy.\\nWe can't open -- can't make a profit\", 'Losing power.\\nThe laser drained it.', 'You mock us with a smile?', \"I've got my Dr. Gruber, says her heart condition means they gave her the wrong anaesthetic anyway, plus she\"]\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=7208)\u001b[0m [\"I know.\\nI'm sorry.\\nI'm sorry.\\nI'm sorry.\\nI'm sorry.\\nI'm sorry.\\nI'm sorry\", \"Nonsense No, I don't know.\"]\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=7208)\u001b[0m [\"I know.\\nI'm sorry.\", 'Nonsense']\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=7203)\u001b[0m ['Uh-huh!']\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=7203)\u001b[0m ['Uh-huh!']\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=7104)\u001b[0m [\"Other than Brill.\\nI'm not sure.\", \"No, you guys are great together.\\nHe'll come through, I'm sure.\\nI'm not wasting my time.\", \"I don't care what you believe.\\nI saved your life once, I don't plan on making a habit of it.\\nYou\"]\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=7104)\u001b[0m ['Other than Brill.', \"No, you guys are great together.\\nHe'll come through, I'm sure.\", \"I don't care what you believe.\\nI saved your life once, I don't plan on making a habit of it.\\nYou\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-05 15:00:39,118 | server.py:182 | evaluate_round 5 received 5 results and 0 failures\n",
      "DEBUG flwr 2023-08-05 15:00:39,120 | server.py:218 | fit_round 6: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=7138)\u001b[0m [\"... That's funny...\\nI had this job with Enzo... we got fired too.\", \"Five and a half minutes.\\nHe's breached the hot zone..\\nI'm not going to be able to get him.\", \"No.\\nNever heard of him.\\nI'm not sure.\", \"A what?\\nI'm a paleontologist.\", \"What was the color of Tim's belt?\\nI'm not sure.\\nI'm not sure.\\nI'm not sure.\\nI'm not sure.\\nI'm not sure.\\nI'm not sure.\\nI'm not sure.\\nI'm not sure.\\nI\", \"No.\\nHeadin' west.\\nTo Can-tuck-ee.\\nI'm not going to bet on it.\"]\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=7138)\u001b[0m [\"... That's funny...\\nI had this job with Enzo... we got fired too.\", \"Five and a half minutes.\\nHe's breached the hot zone..\", 'No.\\nNever heard of him.', 'A what?', \"What was the color of Tim's belt?\", \"No.\\nHeadin' west.\\nTo Can-tuck-ee.\"]\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=12386)\u001b[0m Training Started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=12386)\u001b[0m You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=12386)\u001b[0m /home/qudgks/miniconda3/envs/FL/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=12386)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=12441)\u001b[0m /home/qudgks/miniconda3/envs/FL/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=12441)\u001b[0m   warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create FedAvg strategy\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1.0,  # Sample 100% of available clients for training\n",
    "    fraction_evaluate=1.0,  # Sample 50% of available clients for evaluation\n",
    "    min_fit_clients=5,  # Never sample less than 10 clients for training\n",
    "    min_evaluate_clients=5,  # Never sample less than 5 clients for evaluation\n",
    "    min_available_clients=5,  # Wait until all 10 clients are available\n",
    ")\n",
    "\n",
    "# Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)\n",
    "client_resources = None\n",
    "if DEVICE.type == \"cuda\":\n",
    "    client_resources = {\"num_gpus\": 1}\n",
    "\n",
    "NUM_CLIENTS = 5\n",
    "# Start simulation\n",
    "history = fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=10),\n",
    "    strategy=strategy,\n",
    "    client_resources=client_resources,\n",
    "#     ray_init_args={\"include_dashboard\": False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b31fb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "History (loss, distributed):\n",
       "\tround 1: 0.710209068776628\n",
       "\tround 2: 0.6990351186853316\n",
       "\tround 3: 0.6785576384662204\n",
       "\tround 4: 0.6928872793670116\n",
       "\tround 5: 0.6753511868533172\n",
       "\tround 6: 0.6980922253922968\n",
       "\tround 7: 0.6756048691418136\n",
       "\tround 8: 0.6608476708074534\n",
       "\tround 9: 0.7113708460133902\n",
       "\tround 10: 0.6638261107729764"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d2393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base line\n",
    "History (loss, distributed):\n",
    "\tround 1: 0.710209068776628\n",
    "\tround 2: 0.6990351186853316\n",
    "\tround 3: 0.6785576384662204\n",
    "\tround 4: 0.6928872793670116\n",
    "\tround 5: 0.6753511868533172\n",
    "\tround 6: 0.6980922253922968\n",
    "\tround 7: 0.6756048691418136\n",
    "\tround 8: 0.6608476708074534\n",
    "\tround 9: 0.7113708460133902\n",
    "\tround 10: 0.6638261107729764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix round 10\n",
    "History (loss, distributed):\n",
    "\tround 1: 0.7222987827145466\n",
    "\tround 2: 0.7267468654899574\n",
    "\tround 3: 0.7276815581253804\n",
    "\tround 4: 0.7292324406573341\n",
    "\tround 5: 0.7272822276323798\n",
    "\tround 6: 0.729241387705417\n",
    "\tround 7: 0.7286804625684723\n",
    "\tround 8: 0.7295257455873402\n",
    "\tround 9: 0.7313477175897748\n",
    "\tround 10: 0.7306583688374925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec5cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#round 2 prefix\n",
    "History (loss, distributed):\n",
    "\tround 1: 0.7422124581175754\n",
    "\tround 2: 0.7481604629911666\n",
    "# round 2 baseline\n",
    "History (loss, distributed):\n",
    "\tround 1: 0.7218715199512641\n",
    "\tround 2: 0.7248902223575997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd7141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def weighted_average(metrics):\n",
    "#     accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "#     losses = [num_examples * m[\"loss\"] for num_examples, m in metrics]\n",
    "#     examples = [num_examples for num_examples, _ in metrics]\n",
    "#     return {\"accuracy\": sum(accuracies) / sum(examples), \"loss\": sum(losses) / sum(examples)}\n",
    "\n",
    "# strategy = fl.server.strategy.FedAvg(\n",
    "#     fraction_fit=1.0,\n",
    "#     fraction_evaluate=1.0,\n",
    "#     evaluate_metrics_aggregation_fn=weighted_average,\n",
    "# )\n",
    "\n",
    "# fl.simulation.start_simulation(\n",
    "#     client_fn=client_fn,\n",
    "#     num_clients=NUM_CLIENTS,\n",
    "#     config=fl.server.ServerConfig(num_rounds=NUM_ROUNDS),\n",
    "#     strategy=strategy,\n",
    "#     client_resources={\"num_cpus\": 1, \"num_gpus\": 0},\n",
    "#     ray_init_args={\"log_to_driver\": False, \"num_cpus\": 1, \"num_gpus\": 0}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hostory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996283b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FL",
   "language": "python",
   "name": "fl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
